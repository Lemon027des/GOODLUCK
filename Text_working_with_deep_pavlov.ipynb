{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c3c9d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, Trainer, TrainingArguments, AutoModelForSequenceClassification\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0f7f86",
   "metadata": {},
   "source": [
    "## Вставляй свои пути!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train (19).csv') # !!!\n",
    "test = pd.read_csv('test (24).csv') # !!!\n",
    "ss = pd.read_csv('sample_submission (26).csv') # !!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836c9b3c",
   "metadata": {},
   "source": [
    "## Выбирай тут модель (начинай с deep_pavlov base, переходи у более крупным)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a72aba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'DeepPavlov/rubert-base-cased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03339feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name,  num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b76a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "train_df = Dataset.from_pandas(pd.DataFrame({'text': train['text'] , 'labels': train['target']}))\n",
    "# СВОЕ ВСТАВЛЯЙЙ!!!!\n",
    "test_df = Dataset.from_pandas(pd.DataFrame({'text': test['text']}))\n",
    "# аналогично\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048796b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(batch):\n",
    "    return tokenizer(batch['text'], truncation=True, padding='longest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a685310",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_t = train_df.map(tokenize, batched=True)\n",
    "\n",
    "test_df_t = test_df.map(tokenize, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d293d108",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_args = TrainingArguments(\n",
    "    output_dir='idk',\n",
    "    num_train_epochs=3, # если A100 фигачит быстро, то поставь побольше хз\n",
    "    per_device_train_batch_size=16, # и тут аналогичноо\n",
    "    learning_rate=3e-5,\n",
    "    logging_steps=500,\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"idk\",\n",
    "    report_to=\"none\",\n",
    "    bf16=True, #ибо A100б\n",
    "    warmup_ratio=0.1,\n",
    "    weight_decay=0.01\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353d568a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9a0c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = np.argmax(pred.predictions, axis=-1)\n",
    "    return {'accuracy': accuracy_score(labels, preds)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90b49c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=train_args,\n",
    "    train_dataset=train_df_t,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2f43c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = trainer.predict(test_df_t)\n",
    "y_pred = np.argmax(preds.predictions, axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d076655",
   "metadata": {},
   "source": [
    "# разные уровни очистки, лучше просто оставь 1 уровень"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309574cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from typing import List\n",
    "\n",
    "# -----------------------------\n",
    "# LEVEL 0: минимальная обработка\n",
    "# -----------------------------\n",
    "def preprocess_level_0(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Удаляем только невидимые символы и лишние пробелы.\n",
    "    Это полностью безопасно для RuBERT.\n",
    "    \"\"\"\n",
    "    text = text.replace(\"\\n\", \" \").replace(\"\\t\", \" \")\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# LEVEL 1: лёгкая очистка\n",
    "# -----------------------------\n",
    "def preprocess_level_1(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Уровень 0 + удаление URL, упрощение повторов пробелов.\n",
    "    \"\"\"\n",
    "    text = preprocess_level_0(text)\n",
    "    \n",
    "    # Удаление ссылок\n",
    "    text = re.sub(r\"http\\S+|www\\.\\S+\", \"\", text)\n",
    "    \n",
    "    # Удаление мусорных спецсимволов\n",
    "    text = re.sub(r\"[«»¬]\", \"\", text)\n",
    "    return text\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# LEVEL 2: умеренная очистка\n",
    "# -----------------------------\n",
    "def preprocess_level_2(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Уровень 1 + удаление эмодзи + трансформация повторов знаков.\n",
    "    \"\"\"\n",
    "    text = preprocess_level_1(text)\n",
    "\n",
    "    # Удаление emoji\n",
    "    emoji_pattern = re.compile(\n",
    "        \"[\"  \n",
    "        \"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        \"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        \"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        \"\\U0001F1E0-\\U0001F1FF\"  # flags\n",
    "        \"]+\", flags=re.UNICODE)\n",
    "    text = emoji_pattern.sub(\"\", text)\n",
    "\n",
    "    # Сведение !!! или ??? к одному знаку\n",
    "    text = re.sub(r\"!{2,}\", \"!\", text)\n",
    "    text = re.sub(r\"\\?{2,}\", \"?\", text)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# LEVEL 3: глубокая очистка\n",
    "# -----------------------------\n",
    "def preprocess_level_3(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Уровень 2 + удаление пунктуации, кроме важных знаков.\n",
    "    \"\"\"\n",
    "    text = preprocess_level_2(text)\n",
    "\n",
    "    # Удаляем всю пунктуацию, оставляем только . ! ?\n",
    "    allowed = \".!?\"\n",
    "    text = \"\".join(ch for ch in text if ch.isalnum() or ch.isspace() or ch in allowed)\n",
    "\n",
    "    # Удаление 2+ пробелов\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# LEVEL 4: агрессивная нормализация\n",
    "# -----------------------------\n",
    "import pymorphy2\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "def preprocess_level_4(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Уровень 3 + приведение к нижнему регистру + лемматизация.\n",
    "    ⚠ Может ухудшать качество BERT (cased модель!).\n",
    "    \"\"\"\n",
    "    text = preprocess_level_3(text)\n",
    "\n",
    "    # Нижний регистр\n",
    "    text = text.lower()\n",
    "\n",
    "    # Лемматизация\n",
    "    tokens = text.split()\n",
    "    lemmas = []\n",
    "    for token in tokens:\n",
    "        parsed = morph.parse(token)\n",
    "        if parsed:\n",
    "            lemmas.append(parsed[0].normal_form)\n",
    "        else:\n",
    "            lemmas.append(token)\n",
    "\n",
    "    return \" \".join(lemmas)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52c9b3f",
   "metadata": {},
   "source": [
    "# а лучше вообще вот это"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b848648",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"Очистка текста\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "            \n",
    "    # Приведение к нижнему регистру\n",
    "    text = text.lower()\n",
    "        \n",
    "    # Удаление лишних пробелов\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        \n",
    "        # Удаление URL\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "        \n",
    "        # Удаление email\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "        \n",
    "        # Удаление лишних знаков препинания (можно настроить)\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "        \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da26b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocess_medium(text: str) -> str:                         # убрать пробелы по краям\n",
    "    text = re.sub(r'\\s+', ' ', text)              # один пробел вместо множества\n",
    "    text = re.sub(r'[^\\S\\r\\n]+', ' ', text)       # убрать \"странные\" пробелы\n",
    "\n",
    "    text = text.lower()                           # привести к нижнему регистру\n",
    "    text = re.sub(r'https?://\\S+', '', text)      # удалить ссылки\n",
    "    text = re.sub(r'@\\w+', '', text)              # удалить упоминания @user\n",
    "    text = re.sub(r'#\\w+', '', text)              # удалить хештеги\n",
    "    text = re.sub(r'[^\\w\\s,.!?—-]', ' ', text)    # убрать всё, кроме рус/англ букв и пунктуации\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd08a00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "02036db6",
   "metadata": {},
   "source": [
    "## вот полный код для копирования"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c523f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re  # Добавь эту строку!\n",
    "from transformers import AutoTokenizer, Trainer, TrainingArguments, AutoModelForSequenceClassification\n",
    "from datasets import Dataset\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "train = pd.read_csv('train (19).csv') # !!!\n",
    "test = pd.read_csv('test (24).csv') # !!!\n",
    "ss = pd.read_csv('sample_submission (26).csv') # !!!\n",
    "\n",
    "model_name = 'DeepPavlov/rubert-base-cased'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name,  num_labels=2)\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Очистка текста\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "            \n",
    "    # Приведение к нижнему регистру\n",
    "    text = text.lower()\n",
    "        \n",
    "    # Удаление лишних пробелов\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        \n",
    "    # Удаление URL\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "        \n",
    "    # Удаление email\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "        \n",
    "    # Удаление лишних знаков препинания (можно настроить)\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "        \n",
    "    return text\n",
    "\n",
    "# ОЧИСТКА ТЕКСТА - ДОБАВЬ ЭТО!\n",
    "train['text'] = [clean_text(text) for text in train['text']]\n",
    "test['text'] = [clean_text(text) for text in test['text']]\n",
    "\n",
    "train_df = Dataset.from_pandas(pd.DataFrame({'text': train['text'], 'labels': train['target']}))\n",
    "test_df = Dataset.from_pandas(pd.DataFrame({'text': test['text']}))\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch['text'], truncation=True, padding='longest')\n",
    "\n",
    "train_df_t = train_df.map(tokenize, batched=True)\n",
    "test_df_t = test_df.map(tokenize, batched=True)\n",
    "\n",
    "train_args = TrainingArguments(\n",
    "    output_dir='idk',\n",
    "    num_train_epochs=3, # если A100 фигачит быстро, то поставь побольше хз\n",
    "    per_device_train_batch_size=16, # и тут аналогичноо\n",
    "    learning_rate=3e-5,\n",
    "    logging_steps=500,\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"idk\",\n",
    "    report_to=\"none\",\n",
    "    bf16=True, #ибо A100\n",
    "    warmup_ratio=0.1,\n",
    "    weight_decay=0.01\n",
    ")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = np.argmax(pred.predictions, axis=-1)\n",
    "    return {'accuracy': accuracy_score(labels, preds)}\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=train_args,\n",
    "    train_dataset=train_df_t,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "trainer.train()\n",
    "\n",
    "preds = trainer.predict(test_df_t)\n",
    "y_pred = np.argmax(preds.predictions, axis=-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046d0d3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab54dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_basic(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    text = re.sub(r'#\\w+', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10389713",
   "metadata": {},
   "source": [
    "# Это брать эмбеддинги"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3be056",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104747392378707854450026582898719863709418306815890313402740053127822796836683271389039595147823857890908007777883475087934619687833828178794258094782112100900332372110449971817917771254103698862195189118289090000958828318616560641053649361098556060632476357267504984509875667353288472086571911738355158809676752759193418985526202606998652829066841557447549634547296354634561531871549265920422311386749583036985277299771150068364971766125974389653732139336783600510146039122083280106322636585017416326516782709714066810199255558422633797300445700887541426446807604009524489994469722326666771284755273288634092881963662578131246973608775195143182878068231386434937077250991831047433026453950696550109527950655256144246361533825095748635497236650139961487328075736452988272621398312188721619769863435339838756621098336946625196004509286706731635829189077194513750330316171044705921780962692365343168516092574400026504245064095215998511095999901489247157929706305684046787151520003532276033770237946341930877921990093633342157640135798064354857706521306033910265656603611604592495339930493701735628596344524439232806875090505800801398446167081"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "# Шаг 1: Загрузка модели и токенизатора\n",
    "model_name = \"DeepPavlov/rubert-base-cased\"  # или \"DeepPavlov/rubert-base-cased\" для русского\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# Шаг 2: Подготовка текста\n",
    "text = \"Hello, how are you today?\"\n",
    "# или для нескольких текстов:\n",
    "texts = [\"First sentence\", \"Second sentence here\"]\n",
    "\n",
    "# Шаг 3: Токенизация\n",
    "inputs = tokenizer(\n",
    "    text, \n",
    "    return_tensors=\"pt\",           # Возвращать PyTorch tensors\n",
    "    padding=True,                  # Дополнять до одинаковой длины\n",
    "    truncation=True,               # Обрезать слишком длинные последовательности\n",
    "    max_length=512,                # Максимальная длина\n",
    "    add_special_tokens=True        # Добавлять [CLS] и [SEP]\n",
    ")\n",
    "\n",
    "print(\"Токенизированный вход:\")\n",
    "print(f\"input_ids: {inputs['input_ids']}\")\n",
    "print(f\"attention_mask: {inputs['attention_mask']}\")\n",
    "\n",
    "# Переводим модель в режим оценки (важно для consistency)\n",
    "model.eval()\n",
    "\n",
    "# Отключаем вычисление градиентов для экономии памяти\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# last_hidden_state - это то, что нам нужно!\n",
    "last_hidden_state = outputs.last_hidden_state\n",
    "\n",
    "print(f\"\\nФорма last_hidden_state: {last_hidden_state.shape}\")\n",
    "# Для одного предложения: [batch_size, sequence_length, hidden_size]\n",
    "# Пример: torch.Size([1, 9, 768])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b50b39d",
   "metadata": {},
   "source": [
    "# Усреднение векторов для CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166e999a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def get_sentence_embedding(texts, model_name='DeepPavlov/rubert-base-cased'):\n",
    "    \"\"\"Получает эмбеддинги предложений через усреднение токенов\"\"\"\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name)\n",
    "    \n",
    "    # Токенизация\n",
    "    inputs = tokenizer(\n",
    "        texts, \n",
    "        return_tensors=\"pt\", \n",
    "        padding=True, \n",
    "        truncation=True, \n",
    "        max_length=512\n",
    "    )\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    last_hidden_state = outputs.last_hidden_state\n",
    "    attention_mask = inputs['attention_mask']\n",
    "    \n",
    "    # Усреднение с учетом маски\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "    sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n",
    "    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "    mean_embeddings = sum_embeddings / sum_mask\n",
    "    \n",
    "    return mean_embeddings.numpy()\n",
    "\n",
    "\n",
    "texts = [\"This is first text\", \"Another text for classification\"]\n",
    "embeddings = get_sentence_embedding(texts)\n",
    "\n",
    "print(f\"Форма эмбеддингов для CatBoost: {embeddings.shape}\")\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d179ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install sentence-transformers\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "def get_sbert_embeddings(texts, model_name='all-MiniLM-L6-v2'):\n",
    "    \"\"\"Использует специализированные модели для эмбеддингов\"\"\"\n",
    "    model = SentenceTransformer(model_name)\n",
    "    embeddings = model.encode(texts)\n",
    "    return embeddings\n",
    "\n",
    "# Использование\n",
    "sbert_embeddings = get_sbert_embeddings(texts)\n",
    "print(f\"Форма SBERT эмбеддингов: {sbert_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f130d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def get_embeddings(texts):\n",
    "    tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "    model = AutoModel.from_pretrained('bert-base-uncased')\n",
    "    \n",
    "    inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Усредняем эмбеддинги токенов\n",
    "    mask = inputs['attention_mask'].unsqueeze(-1)\n",
    "    embeddings = (outputs.last_hidden_state * mask).sum(1) / mask.sum(1)\n",
    "    \n",
    "    return embeddings.numpy()\n",
    "\n",
    "# Использование\n",
    "X = get_embeddings(your_texts)\n",
    "# Теперь X готов для CatBoost!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
