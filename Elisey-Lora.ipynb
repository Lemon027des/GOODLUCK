{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ebc2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, math, numpy as np\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130bd1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "    test = pd.read_csv('/kaggle/input/jigsaw-agile-community-rules/test.csv')\n",
    "    sub = pd.read_csv('/kaggle/input/jigsaw-agile-community-rules/sample_submission.csv', index_col='row_id')\n",
    "else:\n",
    "    test = pd.read_csv('/kaggle/input/jigsaw-agile-community-rules/train.csv')\n",
    "    sub = test[['row_id']].copy()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b74aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import vllm\n",
    "\n",
    "llm = vllm.LLM(\n",
    "    \"/kaggle/input/jigsaw-llama3-1-8b-instruct-training/llama-8b-instruct-jigsaw\",\n",
    "    tensor_parallel_size=2, \n",
    "    gpu_memory_utilization=0.95, \n",
    "    trust_remote_code=True,\n",
    "    dtype=\"half\", \n",
    "    enforce_eager=True,\n",
    "    max_model_len=2048,\n",
    "    # disable_log_stats=True,\n",
    "    # enable_prefix_caching=True,\n",
    "    \n",
    ")\n",
    "tokenizer = llm.get_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ab4c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, List\n",
    "from transformers import LogitsProcessor\n",
    "import torch\n",
    "\n",
    "choices = [\"No\", \"Yes\"]\n",
    "\n",
    "KEEP = []\n",
    "for x in choices:\n",
    "    c = tokenizer.encode(x,add_special_tokens=False)[0]\n",
    "    KEEP.append(c)\n",
    "print(f\"Force predictions to be tokens {KEEP} which are {choices}.\")\n",
    "\n",
    "class DigitLogitsProcessor(LogitsProcessor):\n",
    "    def __init__(self, tokenizer):\n",
    "        self.allowed_ids = KEEP\n",
    "        \n",
    "    def __call__(self, input_ids: List[int], scores: torch.Tensor) -> torch.Tensor:\n",
    "        scores[self.allowed_ids] += 100\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee208d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys_prompt = '''You are given a comment on reddit and a rule. Your task is to classify whether the comment violates the rule. Only respond Yes/No.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2995267c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting(dataset):\n",
    "    texts = []\n",
    "    for i in range(len(dataset)):\n",
    "        texts.append(tokenizer.apply_chat_template(dataset[i], tokenize=False, add_generation_prompt=False))\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9140b939",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "Subreddit: r/{subreddit}\n",
    "Rule: {rule}\n",
    "Examples:\n",
    "1) {positive_example_1}\n",
    "Violation: Yes\n",
    "\n",
    "2) {negative_example_1}\n",
    "Violation: No\n",
    "\n",
    "3) {negative_example_2}\n",
    "Violation: No\n",
    "\n",
    "4) {positive_example_2}\n",
    "Violation: Yes\n",
    "Comment:\n",
    "{body}\n",
    "Violation: \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8c4327",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "for index,row in test.iterrows():\n",
    "    \n",
    "    formatted_sample = [\n",
    "        {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": sys_prompt\n",
    "    },\n",
    "       {\n",
    "           \"role\": \"user\",\n",
    "           \"content\": template.format(\n",
    "               rule = row.rule,\n",
    "               subreddit = row.subreddit,\n",
    "               body = row.body,\n",
    "               positive_example_1 = row.positive_example_1,\n",
    "               negative_example_1 = row.negative_example_1,\n",
    "               positive_example_2 = row.positive_example_2,\n",
    "               negative_example_2 = row.negative_example_2\n",
    "           )\n",
    "       }]\n",
    "    \n",
    "    dataset.append( formatted_sample )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739f47e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_prompts = formatting(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4567fc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_processors = [DigitLogitsProcessor(tokenizer)]\n",
    "responses = llm.generate(\n",
    "    all_prompts,\n",
    "    vllm.SamplingParams(\n",
    "        n=1,  # Number of output sequences to return for each prompt.\n",
    "        top_p=0.9,  # Float that controls the cumulative probability of the top tokens to consider.\n",
    "        temperature=0,  # randomness of the sampling\n",
    "        seed=777, # Seed for reprodicibility\n",
    "        skip_special_tokens=True,  # Whether to skip special tokens in the output.\n",
    "        max_tokens=1,  # Maximum number of tokens to generate per output sequence.\n",
    "        logits_processors=logits_processors,\n",
    "        logprobs = 2\n",
    "    ),\n",
    "    use_tqdm = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa503e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "errors = 0\n",
    "\n",
    "for i,response in enumerate(responses):\n",
    "    try:\n",
    "        x = response.outputs[0].logprobs[0]\n",
    "        logprobs = []\n",
    "        for k in KEEP:\n",
    "            if k in x:\n",
    "                logprobs.append( math.exp(x[k].logprob) )\n",
    "            else:\n",
    "                logprobs.append( 0 )\n",
    "                print(f\"bad logits {i}\")\n",
    "        logprobs = np.array( logprobs )\n",
    "        logprobs /= logprobs.sum()\n",
    "        results.append( logprobs )\n",
    "    except:\n",
    "        #print(f\"error {i}\")\n",
    "        results.append( np.array([1/2., 1/2.]) )\n",
    "        errors += 1\n",
    "        \n",
    "print(f\"There were {errors} inference errors out of {i+1} inferences\")\n",
    "results = np.vstack(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e502a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = [x[1] for x in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb47758a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "if not os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "    print(roc_auc_score(test['rule_violation'], probs))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
